---
layout: post
title:  "CH6. Ensemble-Based and Hybrid"
subtitle:   "추천시스템 스터디1팀"
categories: rscsys
tags: rscsys
comments: true
---



# 6.1 Introduction

## Hybrid모델을 구성하게 된 이유

기존3가지모델

1. Collaborative : User Community의 rating사용하는방식
2. Content-based : 단순 User의 rating사용하는방식
3. Knowledge-based : User의 requirements를 사용하는방식

→ 추천방식마다  다양한 model, 다양한 data sources를 사용한다는것이 문제

→ 추천방식마다 강점과 약점이 각각 존재함

⇒ 여러가지 모델을 합쳐서 일괄적으로 좋은 성능을 내는 모델을 구상함

⇒ Hybrid Model

## Hybrid모델을 구성하는 3가지 방법

방법3가지

1. Ensemble design
    - 이전모델의 결과(rating output)를 다음모델의 single output에 결합하는 방식
    - Collaborative-Fittering's output → Content-based's input
    - ex. sequential-model (순차적으로 진행하는경우)

        i=0일때 output을, i=1일때 input으로 넣는것

2. Monolithic design
    - 다양한 data type을 사용해서 모델을 구성하는 방식
    - 방식에 대한 결과(Content-based와 Collaborative중 어떤방식을 사용할지?)보다
    data source를 어떻게 사용할것인지? 가 좀더 영향을 준다
    - 그렇기때문에 각각의 components가 무슨역할을 하는지 쉽게 알수없음
    ( off-the-shelf-black-boxes)
3. Mixed systems
    - 다양한 recommendation알고리즘을 black-boxes로 사용함
    - 각각의 single model의 결과가 의미가없고, model을 결합했을때 의미가 있는 경우에 사용한다

## Hybrid systems의 종류

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b13f03ad-12cd-4c28-a7da-24413a799e0d/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b13f03ad-12cd-4c28-a7da-24413a799e0d/Untitled.png)

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8703da96-b413-4718-ae4a-03990afea2b9/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8703da96-b413-4718-ae4a-03990afea2b9/Untitled.png)

1. Weighted 

    다양한 Component에서 나온 Score의 합계를 Weight을 줘서 sigle score에 결합하는 방식

2. Switching

    전체적인 성능을 위해서 '다음에 어떤방식을 쓸까?'를 결정하는 방식

    ex. knowledgebased→(Content?Collaborative?)→Knowledge→(Content?Collaborative?)

3. Cascade

    이전 모델의 결과가, 다음 모델에 영향을 주는경우

    ex. boosting모델

    ex. previous-model의 Weight ⇒ now-model에 Wieghted ⇒ next-model에 Weighted

    ![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/058d8b28-a62a-4b47-9163-5b8f91e8681f/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/058d8b28-a62a-4b47-9163-5b8f91e8681f/Untitled.png)

4. Feature augmentation

    3.Cascade는 이전모델의 Weight를 쓰는반면

    4.Feature augmentation은 이전모델의 Output를 다음모델의 input feature만들때 사용함

    ex. stacking 모델

5. Feature combination

    다양한 data source에서 feature를 추출후 Combine ⇒ Single recommendation에 사용

6. Meta level

    하나의 모델 자체가 다른 모델의 input임

    모델 내부에서 Context matrix 내부에 있는 peer그룹을 사용하고

    off-the-shelf컨셉은 사용안함

    이러한 개념들 때문에 Context를 통한 Collaborative라고 말하는듯 (Collaborative via context)

7. Mixed 

    여러번의 engines으로 부터의 recommendation이 동시간의 user를 나타내는 방식...?

    다양한 component로부터 score를 결합하는것이 아니기 때문에 '정확히는 ensemble 시스템'이 아니다

    다양한 item간 연관된 entity이 존재하는경우 ⇒ 연관성이 있는경우라고 함

    이때의 Model 은 Black-Box

    But. 다양한 Model의 Output을 Combine하지않음 ⇒ Ensemble이 아니라  Multiple recommendation에 가까움

## 4.Feature augmentation 과 7.Mixed

7.Mixed가 Monolithic과 Ensemble이지 않은 이유

7.Mixed는 entity의 결합 ⇒ 다량의 recommendation이 사용되기때문

4.Feature augmentation이 Monolithic보다는 Ensemble인 이유

4.Feature augmentation에서 각각의 components가 off-the-shelf black-boxes로 사용되고있기때문에 monolithic이다.

근데 4.Feature augmentation은 Classfication에서의 Stacking method를 연관시키기때문에 Feature Agumentation은 Monolithic보다는 Ensemble이라고 함

## Stacking Method (Stacked Generalization)

의도 : Model의 Overfitting을 막기위해서사용

다른점

Bagging과 다른점 : 각각의 모델은 전부 다 다른모델로 사용함 + 동일한 Data셋트를 사용

Boosting과 다르점 : 다양한 모델의 결합하는게 아니라, 예측잘하는 '단일'모델을 사용함

구성 : Level 0 model(Base-Model) + Level 1 model(Meta-Model)

Level0 Model : Training Data를 잘 예측하도록 컴파일된 모델

Level1 Model : Level0 Model을 어떻게 결합해야. 모델 성능이 좋을지를 학습하는 모델

## Classfication vs Collaborative

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/102363a1-573a-43ad-87e8-5148605aa244/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/102363a1-573a-43ad-87e8-5148605aa244/Untitled.png)

Classfication

1. Data가 구별되어있고(Independet?Dependent?),(Training?Test?)
2. Missing data(회색부분) 이 모여있음

Collaborative

1. Data가 구별되어있지않고(no demarcation)
2. Missing data(회색부분)이 모여있지않음

# 6.2 Ensemble Methods from the Classfication Perspective

6.2에서 Classfication을 대상으로 하는 이유 :

Classfication에서는 feature와 class구분이 잘 되어있기때문에 결과가 비교적 분명함

Collaborative에서는 missing entries때문에 결과가 상대적으로 덜 불분명함

그리고 사실 Classfication은 충분히 Collaborate로 generalize가 가능하기때문임.

## Bias-Variance trade off  (편향-분산 tradeoff)딜레마

Bias 

- 편향은 학습 알고리즘에서 잘못된 가정을 했을 때 발생하는 오차이다.
- 장 : 고편향값은 Overfitting할 일 없이 단순한 모델을 제시
- 단 : 고편향값은 Training Data으로부터 규칙성을 제대로 포착하지못함 ⇒ Underfitting

Variance

- 분산은 트레이닝 셋에 내재된 작은 변동(fluctuation) 때문에 발생하는 오차이다.
- 장 : 고분산값은 Training set을 잘 표현한다
- 단 : 고분산값은 지나치게 큰 Noise, 부적절한 Training Data까지 과적합 ⇒ Overfitting

Bias-Variance관계

- 학습알고리즘의 error값을 분석하는 한가지 방법
- error값이 bias, variance, data로부터 나온것이기때문에 줄일수없는 오류의 합으로 본다

⇒ 모델의 Accuracy를 높이기위해서 Model을 multiple-Collaborative로 만들면 bias-variance관계가 형성됨

- Collaborative모델은 결국 model이므로 → bias존재
- 학습시키는 Data에 따라서 모델 결과가 달라짐(dependency)
- 학습시키는 Data가 가지고있는 본질(intrinsic) 때문에 Data는 Noise를 가지고있을수밖에없음

Error = Bias^2 + Variance + Noise

- 이때 Noise는 Data의 본질적인 특성이기때문에 바꿀수없음

⇒ 따라서 Bias와 Variance를 줄이는 방식으로 진행함

Classfication(bagging방식) → Variance를 줄이는 방식으로 error를 줄임

Colaborative(boosting방식) → Vias를 줄이는 방식으로 error를 줄임

# 6.3 Weighted Hybrid

## 일반적인 수식 R^

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d9be7818-44c9-4d1e-9365-c047fed6567f/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d9be7818-44c9-4d1e-9365-c047fed6567f/Untitled.png)

R^ 은 Weight와 그때의 R^의 합으로 계산된다

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/eb3ee4a6-7eae-4463-85cc-cfa1c5e8ac55/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/eb3ee4a6-7eae-4463-85cc-cfa1c5e8ac55/Untitled.png)

R = m X n 

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/86eb6713-7546-4623-9e10-9a74f2c262e3/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/86eb6713-7546-4623-9e10-9a74f2c262e3/Untitled.png)

q : 서로다른 알고리즘의 종류

모델의 weight집합 alpha

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/60039eff-f733-40ad-8ce8-517999868f89/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/60039eff-f733-40ad-8ce8-517999868f89/Untitled.png)

R^1 ... R^q : R의 unobserve되있는 부분에 알고리즘q를 적용했을때 나오는 결과Matrix

## Weight가 모두 같을때  r^uj

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9d96e51f-2eb1-4af9-abd7-a59094c1ecee/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9d96e51f-2eb1-4af9-abd7-a59094c1ecee/Untitled.png)

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8f9bf55b-9678-4640-8d8d-4aa1933b0b00/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8f9bf55b-9678-4640-8d8d-4aa1933b0b00/Untitled.png)

r^uj는 final prediction을 나타낸다

i번째 앙상블 컴포넌트

u : user

j : item

이때 중요한 내용 : weight에 대한 검증과정이 필요함

→ 자세한 접근 : 7장

## Weight에 대한 간단한 검증과정

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2633240e-2f81-4be3-b317-4876ed780495/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2633240e-2f81-4be3-b317-4876ed780495/Untitled.png)

관측되는 Matrix를 기준으로

1 : unobserved data

2,3 : observed data

2 : Observed data의 25% (예시)

3 : Observed data의 75% (예시)

Observed 되는 부분을 2와 3으로 분류해서

2는 그대로 유지

3에 서로다른 q개의 알고리즘을 적용한 결과 matrix R^1 ~ R^q를 생성함

생성된 R^1 ~ R^q를 일반적인 수식 R^에다가 적용하는 방법이 있음.

이때 R^1 ~ R^q에 대한 weight의 vector를 a_ 이라고함

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6910fa67-63b7-4220-95bc-6acfd760a064/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6910fa67-63b7-4220-95bc-6acfd760a064/Untitled.png)

a_ 는 a1~ aq까지 존재함(R^q까지 존재하기떄문에)

⇒ 이에대한 수치가 MSE, MAE임

## MSE, MAE정의

MSE : Mean Square Error

MAE : Mean Absolute Error

일반적으로 회귀모델에서 정의되는 MSE와 MAE는

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ecde892a-8677-4415-9ff0-323e94d79f42/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ecde892a-8677-4415-9ff0-323e94d79f42/Untitled.png)

MSE : 제곱(면적)

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5211eb47-b6d2-49d4-bb21-9c6391a53335/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5211eb47-b6d2-49d4-bb21-9c6391a53335/Untitled.png)

MAE : 절댓값

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/48d6a9a6-4024-4eea-85be-5faceb87a9dc/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/48d6a9a6-4024-4eea-85be-5faceb87a9dc/Untitled.png)

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ea5281a3-5b65-4db1-83c2-dfcf79c6b863/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ea5281a3-5b65-4db1-83c2-dfcf79c6b863/Untitled.png)

## MSE, MAE의 적용

H :실제 데이터 관측치 ( raw value라고 생각하면 될듯)

a_ : Independent variables

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/fa7cef80-e348-4c8d-9225-2e3cfae19525/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/fa7cef80-e348-4c8d-9225-2e3cfae19525/Untitled.png)

a_는 weight이기때문에 a_가 최소화 되는 방향으로 진행함

아이디어 : Independent variable은 MSE를 이용해서 minimize화를 진행하는것이 가능하다 

→ H(실제 데이터 관측치) 를 통해서 respect value(기댓값)을 구할수 있을것

## 어떻게 Variable의 dependent? Independent가 정해지는가?

- Independent variable(독립변수) → rating predictions of various models for entry(u,j)
- Dependent variable(종속변수) → H에서의 value of each predicted rating r^uj의 결과값

Linear regression과정에서 Data에 대한 Train이 끝나면

individual component가 다시 Data를 Train하는 과정을 가진다(retrain)

⇒ 이 내용은 Cross-validation에서 진행되는 내용이고 CH7에서 다룰예정

## Linear Regression에서의 Gradient MAE사용

문제점

Linear Regression 에서는 outliar에 대한 영향이 크다

이때 MSE를 사용하면 망함

→ MSE대신 MAE를 사용함

MAE는 large error를 도출해 내지않아서 가능함

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3a3f0eca-2df0-4b68-865e-0dbe8b607f8a/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3a3f0eca-2df0-4b68-865e-0dbe8b607f8a/Untitled.png)

일때

Gradient

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c5bc6e78-ce23-4b26-8bee-dd9522e5f48d/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c5bc6e78-ce23-4b26-8bee-dd9522e5f48d/Untitled.png)

value of r^uj

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/7c1fd483-6c9f-4eea-a571-4ac6466c9669/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/7c1fd483-6c9f-4eea-a571-4ac6466c9669/Untitled.png)

MAE의 Gradient수식은

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/12769ee3-2d38-4e57-8c11-cf9ec20cdce0/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/12769ee3-2d38-4e57-8c11-cf9ec20cdce0/Untitled.png)

Iterative 루틴

Initialize(초기화)

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e95a32c4-2e36-45c6-abca-076cf1b9de59/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e95a32c4-2e36-45c6-abca-076cf1b9de59/Untitled.png)

1. Update a

    ![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c6eefefd-8b4e-443a-9576-d225412a4121/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c6eefefd-8b4e-443a-9576-d225412a4121/Untitled.png)

    이때 r>0 이고 MAE가 증가할것임

2. Update t

    ![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3e4c0763-dc35-419b-9575-c7de07e1abf2/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3e4c0763-dc35-419b-9575-c7de07e1abf2/Untitled.png)

3. Check MAE

    MAE값 확인해서 MAE가 커지면 중단

    MAE가 작아진거면 1부터 다시반복

## Overfitting을 막는 다른방법

- Regularization(데이터 정규화)를 하면 Overfitting을 막을수 있음
- non-negativity값을 가지거나, 확률(Sum to 1)이 되는 값에는 또다른 규제(constraint)를 정할수있음
- Train과 Test의 Data를 8:2정도 비율로 조정하는 hold-out방식을 사용할수도 있음

⇒ 이런식의 방법들은 unseen entries에 대한 generalizability를 증가시킴

# 6.3.1. Various Types of Model Combinations

Model Combinations 에는 2가지 유형이 존재한다

1. Homo-genous data type and  model classes (동질 데이터)
    - 동질의 Data를 사용한다(유사성높음)
    - 동일한 Data에 서로다른 Model을 적용하는것이 가능하다.
    1. 모델 자체를 바꾸는경우

        ex.) neighborhood-based, SVD, Bayes ..

        동일한 Data에 대해서 모델만 바꾸는것이 가능하다.

        장점:  주어진 Dataset에 대해서 Bias이상치를 피할수 있다 → high-quality의 result를 도출가능하다.

        특히 정규행렬인수화, non-negative행렬 인수화, 최대margin인수화에서 좋은결과가 있었음

    2. 모델의 parameter만 바꾸는경우
    - ex.) k-NN
    - 동일한 Data에 대해서 모델은 동일하나, 모델의 인수를 바꿈 (k-NN에서 k를 바꾼다든가)
    - 장점 : 기본 모델의 성능을 향상시킬수 있다.

2. Hetero-genous data type and model classes (이질 데이터)
    - 서로다른 Data를 사용한다(유사성낮음)
    - 서로다른 Data에 서로다른 Model을 적용하는게 가능함 (Homo-genous와 동일)
    - ex.) Model = Component1(CF) + Component2(Content-based)
    - 장점: 다양한 Data source를 토대로 Combine할때 Data에대한 정보(사전지식)을 사용해서 가장 정확한 recommend가 가능함 (knowledge-based모델을 의미하는것같음)
    - 주의사항 : 각각의 Component의 비중을 잘 구성하는것이 중요함

# 6.3.2 Adapting Bagging from Classification

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/058d8b28-a62a-4b47-9163-5b8f91e8681f/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/058d8b28-a62a-4b47-9163-5b8f91e8681f/Untitled.png)

앞서말한 Bias-Variance trade-off는 CF(Collaborative-filltering)의 문제를 가지고있다.

(이유 : CF는 결국 Classfication으로 정규화(generalize)될수 있고, Model이기때문에 error가 존재)

해결방법 : Bagging을 사용해서 error를 줄이는방법으로 진행하기.

## Bagging모델 컨셉

- CF(Collaborative-filltering)에서 Bagging을 쓰고 싶다고해서, 그냥 그대로 쓸수는없음
- Bagging화 하는 과정에서 값이 살짝 달라져야함 →  Model에 따로 Weight를 주는방식으로 해결함

기본 아이디어 : Variance를 줄이는 방향으로 진행

방법 : q-training dataset을 기존 data에서 boostrapped sampling해서 생성

- Bootstrapped Sampling
    - 모집단은 동일한 상태에서, 여러번 random추출하는것 ( 표본을 많이 추출할수있음)

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d9d1690e-fa20-43c6-9668-97544ab256a5/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d9d1690e-fa20-43c6-9668-97544ab256a5/Untitled.png)

- 이때 추출되는 Data matrix의 row의 크기는 원본training dataset의 크기와 같아야함
- 이 과정을 여러번 반복 → 데이터의 중복이 무조건 존재함

→ 총 부트스트랩이 e번 진행됬다면

Boostrapped Sampling된 데이터 ≠ 전체데이터의 1/e 

⇒ Variance를 감소시킨다 ⇒ Error가 감소한다

## 다양한 Bagging, Subagging 방법

1. Row-Wise Boostrapping
2. Row-Wise Subsampling
3. Entry-Wise Bagging
4. Entry-Wise Subsampling

R은 Matrix, Entry는 항목, Boostrapping은 Boostrapped samplling을 지칭한다.

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/102363a1-573a-43ad-87e8-5148605aa244/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/102363a1-573a-43ad-87e8-5148605aa244/Untitled.png)

Row-Wise

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/7e82532d-e8b1-48f5-a931-34ab38f87004/Untitled.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/7e82532d-e8b1-48f5-a931-34ab38f87004/Untitled.png)

(Entry-Wise는 반대라고 생각하면 될것같다)

Subsampling이란  

- 빈도수가 높은 Data일수록, 의미있는 Data가 아니라고 판단함

    → 유의미한 결과를 얻기위해서 제거하는것

    희소성을 본다고 생각하면 될것같다.

- 일정 횟수를 하지않으면, 의미가없음

    → 유의미한 결과를 얻기위해서는 최소 몇번이상을 진행해야함.

1. Row-Wise Boostrapping

    R의Row를 기준으로, 차원의크기가 같은 R1~Rq를 만듬(R1~Rq는 Training Dataset이다)

    - 이때 R1~Rq은 Bootstrapped samplling을 진행함

        → random 추출되었으므로 Data의 중복이 존재함 

    - R1~Rq에서 나온 User에 대해서만 item rating을 예측할수있음

         이유 : 추출되지않은 Data(User)를 기반으로는 rating(item)을 예측못함

    ⇒ 추출된 Rating matrix를 토대로 얼마나 중복되었는지를 확인할수 있음

    - q값이 큰경우, 각각의 user는 1-(1/e)^q의 높은 값을 가진 Ensemble Component가 된다

    ⇒ 모든 User가 높은 probability value를 가짐

2. Row-Wise Subsampling

    1.Row-Wise Boostrapping은 Row(행) 을 교체되지만(R1~Rq)

    2.Row-Wise Subsampling은 Row가 교체되지않고 그대로 Sampling됨

    - Rows의 일부는 (0.1~0.5)사이의 값으로 랜덤하게 추출된다.
    - Subsampling의 횟수 (q) 는 모든 row가 표현될수 있도록 10보다 커야함
    - 문제점: 모든 entries를 예측하기 어렵다

        → 더 적은 수의 Components에 대한 평균을 구해서 사용함

    ⇒ 결과론적으로 Variance가 감소하지 않음

3. Entry-Wise Bagging
    - 원본ratings matrix의 항목(Entry)이 서로다른 q개의 알고리즘에 의해서
    새로운 rating matrix R1~Rq를 만들어냄
    - 이 경우에도 마찬가지로 Random samplling되기때문에 표본이 반복추출될것임

    → 항목이 Weight와 연관됨

    ⇒ Weight를 기반으로 Entry를 처리할수 있는 CF(Collaborative-filltering)이 필요함

    - 2.Row-Wise Bagging과 동일하게, 최종적으로는 평균값을 구해서 사용함.

4. Entry-Wise Subsamppling
    - Entry중 일부가 rating matrix R에서 random하게 유지되게 샘플링한 Dataset를 생성함
    - 2.Row-Wise Subsamppling처럼 0.1~0.5 사이에서 Samppling되고, 
    q개의 Dataset을 만든다(R1~Rq)
    - R1~Rq에서의 각 User와, 각 Item은 각각의 subsampled matrix를 나타낸다
    - R1~Rq는 원래의 Training Data의 크기보다 작다
    - 다른 방법들과 동일하게, 최종적으로는 평균값을 구해서 사용함

## 1~4에서 Simple average가 사용되는 이유

- Weight average가 아닌 Simple average(결과값의 평균) 을 사용하는 이유는

    Model Components는 모두 확률적으로 만들어져있다

    ⇒ 즉 Weight는 모두 같기때문임

- 동일한 Data에 대해서 Subsamppling했을때의 결과가 좋다고해서
CF에서도 항상 좋은 결과를 내는것이 아니기 때문
- 전체적으로는 Model의 Accuracy를 증가하는것이 성공했으나
Neightborhood-model의 경우는 예외였음

## Neighbor-hood Model에서 Bagging을 해도 Acc가 높아지지않는이유

결과론적으로는 Neighbor-hood bagging 모델간, 높은 수준의 relation이 존재한다고 예측함

일반적인 Bagging방법 : Benefit을 최대화 하기 위해서는 

Uncorrelated based model사용, Low Bias, High Variance로 구성함

Neighbor-hood Model은 각각의 기본 변수간 correlation이 높음

⇒ randomness injection하는것이 해결법임

# 6.3.3 Randomness Injection

일반적으로는 RandomForest를 Classfication에서 사용할때 쓰는 접근법임

아이디어 : Randomness를 Classfier에 명시적으로 주입하자는 아이디어

방법은 크게 2가지임

1. Injecting randomness into a neighborhood model (k-NN과정)

    k-NN에서 가장 가까운 K개의 이웃을사용하는것이 아닌,

    top- (a*k)개의 이웃을 선택하고, (a*k)로부터 k개의 원소를 무작위로 추출함

    단, a>1

    ⇒ 이러한 방식은 factor를 1/a 로 만들면, Row-Wise Subsampling의 간접적인 변형이기때문에 사용안함

2. Injecting randomness into a matrix factorization model (Matrix Factroize과정)

    factor matrix에서 factor를 무작위로 초기화시킨후에 → Gradient Descent를 적용함

    ⇒ 본질적으로도 무작위임

    초기화 하는 방법이나 순서에 따라서 다른 Solution을 얻고
    종종 이러한 Solution의 결합은 정확한 결과를 도출함

결과적으로 방법1, 방법2 모두 Randomforest와 마찬가지로 Bias에 영향을 받지않고, Variance를 감소할수 있었다 ⇒ Error감소

Bagging이 작동하지않는경우(다양한 predictor끼리의 high level of correlation인경우) 에는 Randomness Injection도 좋은 방법임